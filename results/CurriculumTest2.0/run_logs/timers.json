{
    "name": "root",
    "gauges": {
        "MosquitoBehaviour.Policy.Entropy.mean": {
            "value": 3.1307010650634766,
            "min": 2.471461534500122,
            "max": 3.193983793258667,
            "count": 103
        },
        "MosquitoBehaviour.Policy.Entropy.sum": {
            "value": 926887.875,
            "min": 144570.609375,
            "max": 1100958.75,
            "count": 103
        },
        "MosquitoBehaviour.TimeOutRewards.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 103
        },
        "MosquitoBehaviour.TimeOutRewards.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 103
        },
        "MosquitoBehaviour.CollisionRewards.mean": {
            "value": -1535.1968994140625,
            "min": -2640.12060546875,
            "max": -6.459994792938232,
            "count": 103
        },
        "MosquitoBehaviour.CollisionRewards.sum": {
            "value": -1535.1968994140625,
            "min": -2640.12060546875,
            "max": -6.459994792938232,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Mosquito.KillRewards.mean": {
            "value": 36.0999755859375,
            "min": 0.5,
            "max": 36.0999755859375,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Mosquito.KillRewards.sum": {
            "value": 36.0999755859375,
            "min": 0.5,
            "max": 36.0999755859375,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Mosquito.VictoryRewards.mean": {
            "value": 79.0,
            "min": 1.0,
            "max": 80.0,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Mosquito.VictoryRewards.sum": {
            "value": 79.0,
            "min": 1.0,
            "max": 80.0,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Mosquito.DefeatRewards.mean": {
            "value": 233.0,
            "min": 1.0,
            "max": 233.0,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Mosquito.DefeatRewards.sum": {
            "value": 233.0,
            "min": 1.0,
            "max": 233.0,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.ObjectivePickUpRewards.mean": {
            "value": 493.5997009277344,
            "min": 2.0,
            "max": 493.5997009277344,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.ObjectivePickUpRewards.sum": {
            "value": 493.5997009277344,
            "min": 2.0,
            "max": 493.5997009277344,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.ObjectiveDropOffRewards.mean": {
            "value": 433.9998779296875,
            "min": 1.2000000476837158,
            "max": 433.9998779296875,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.ObjectiveDropOffRewards.sum": {
            "value": 433.9998779296875,
            "min": 1.2000000476837158,
            "max": 433.9998779296875,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.DeathRewards.mean": {
            "value": -180.5,
            "min": -180.5,
            "max": -2.5,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.DeathRewards.sum": {
            "value": -180.5,
            "min": -180.5,
            "max": -2.5,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.VictoryRewards.mean": {
            "value": 233.0,
            "min": 1.0,
            "max": 233.0,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.VictoryRewards.sum": {
            "value": 233.0,
            "min": 1.0,
            "max": 233.0,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.DefeatRewards.mean": {
            "value": 79.0,
            "min": 1.0,
            "max": 80.0,
            "count": 103
        },
        "MosquitoBehaviour.Agent.Camper.DefeatRewards.sum": {
            "value": 79.0,
            "min": 1.0,
            "max": 80.0,
            "count": 103
        },
        "MosquitoBehaviour.Environment.LessonNumber.training_stage_index.mean": {
            "value": 4.0,
            "min": 3.0,
            "max": 4.0,
            "count": 103
        },
        "MosquitoBehaviour.Environment.LessonNumber.training_stage_index.sum": {
            "value": 4.0,
            "min": 3.0,
            "max": 4.0,
            "count": 103
        },
        "MosquitoBehaviour.Environment.EpisodeLength.mean": {
            "value": 2055.2169811320755,
            "min": 482.28045977011493,
            "max": 2889.4,
            "count": 103
        },
        "MosquitoBehaviour.Environment.EpisodeLength.sum": {
            "value": 217853.0,
            "min": 23346.0,
            "max": 301205.0,
            "count": 103
        },
        "MosquitoBehaviour.Self-play.ELO.mean": {
            "value": 1577.795362418297,
            "min": 1418.555786070112,
            "max": 1890.684273452165,
            "count": 103
        },
        "MosquitoBehaviour.Self-play.ELO.sum": {
            "value": 47333.86087254891,
            "min": 22484.32298013374,
            "max": 266130.3831210506,
            "count": 103
        },
        "MosquitoBehaviour.Step.mean": {
            "value": 7679120.0,
            "min": 1559853.0,
            "max": 7679120.0,
            "count": 103
        },
        "MosquitoBehaviour.Step.sum": {
            "value": 7679120.0,
            "min": 1559853.0,
            "max": 7679120.0,
            "count": 103
        },
        "MosquitoBehaviour.Policy.ExtrinsicValueEstimate.mean": {
            "value": -0.0388629212975502,
            "min": -0.19179774820804596,
            "max": 0.2297448068857193,
            "count": 103
        },
        "MosquitoBehaviour.Policy.ExtrinsicValueEstimate.sum": {
            "value": -2.8369932174682617,
            "min": -25.892696380615234,
            "max": 33.77248764038086,
            "count": 103
        },
        "MosquitoBehaviour.Environment.CumulativeReward.mean": {
            "value": -0.6797979958355427,
            "min": -2.1213539864867927,
            "max": 1.259455403671899,
            "count": 103
        },
        "MosquitoBehaviour.Environment.CumulativeReward.sum": {
            "value": -20.39393987506628,
            "min": -138.55102109164,
            "max": 179.62058752775192,
            "count": 103
        },
        "MosquitoBehaviour.Policy.ExtrinsicReward.mean": {
            "value": -0.6797979958355427,
            "min": -2.1213539864867927,
            "max": 1.259455403671899,
            "count": 103
        },
        "MosquitoBehaviour.Policy.ExtrinsicReward.sum": {
            "value": -20.39393987506628,
            "min": -138.55102109164,
            "max": 179.62058752775192,
            "count": 103
        },
        "MosquitoBehaviour.Losses.PolicyLoss.mean": {
            "value": 0.02232569718454033,
            "min": 0.020984009094536304,
            "max": 0.026092931553410987,
            "count": 103
        },
        "MosquitoBehaviour.Losses.PolicyLoss.sum": {
            "value": 0.11162848592270165,
            "min": 0.06645628295838832,
            "max": 0.15346473847515882,
            "count": 103
        },
        "MosquitoBehaviour.Losses.ValueLoss.mean": {
            "value": 0.0031820843415334824,
            "min": 0.001977373901027022,
            "max": 0.014647464247730871,
            "count": 103
        },
        "MosquitoBehaviour.Losses.ValueLoss.sum": {
            "value": 0.015910421707667412,
            "min": 0.006613059735557596,
            "max": 0.08788478548638523,
            "count": 103
        },
        "MosquitoBehaviour.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0002999999999999999,
            "max": 0.00030000000000000003,
            "count": 103
        },
        "MosquitoBehaviour.Policy.LearningRate.sum": {
            "value": 0.0014999999999999998,
            "min": 0.0009,
            "max": 0.0018,
            "count": 103
        },
        "MosquitoBehaviour.Policy.Epsilon.mean": {
            "value": 0.19999999999999996,
            "min": 0.19999999999999996,
            "max": 0.19999999999999998,
            "count": 103
        },
        "MosquitoBehaviour.Policy.Epsilon.sum": {
            "value": 0.9999999999999998,
            "min": 0.5999999999999999,
            "max": 1.1999999999999997,
            "count": 103
        },
        "MosquitoBehaviour.Policy.Beta.mean": {
            "value": 0.005,
            "min": 0.004999999999999999,
            "max": 0.005,
            "count": 103
        },
        "MosquitoBehaviour.Policy.Beta.sum": {
            "value": 0.025,
            "min": 0.015,
            "max": 0.030000000000000002,
            "count": 103
        },
        "MosquitoBehaviour.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 103
        },
        "MosquitoBehaviour.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 103
        },
        "CamperBehaviour.Policy.Entropy.mean": {
            "value": 2.541994571685791,
            "min": 2.2508552074432373,
            "max": 3.2982089519500732,
            "count": 308
        },
        "CamperBehaviour.Policy.Entropy.sum": {
            "value": 144540.359375,
            "min": 57661.23046875,
            "max": 1569506.625,
            "count": 308
        },
        "CamperBehaviour.TimeOutRewards.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 308
        },
        "CamperBehaviour.TimeOutRewards.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 308
        },
        "CamperBehaviour.CollisionRewards.mean": {
            "value": -1912.7579345703125,
            "min": -2726.438720703125,
            "max": -4.209997653961182,
            "count": 308
        },
        "CamperBehaviour.CollisionRewards.sum": {
            "value": -1912.7579345703125,
            "min": -2726.438720703125,
            "max": -4.209997653961182,
            "count": 308
        },
        "CamperBehaviour.Agent.Mosquito.KillRewards.mean": {
            "value": 34.299983978271484,
            "min": 0.9000000357627869,
            "max": 36.69995880126953,
            "count": 308
        },
        "CamperBehaviour.Agent.Mosquito.KillRewards.sum": {
            "value": 34.299983978271484,
            "min": 0.9000000357627869,
            "max": 36.69995880126953,
            "count": 308
        },
        "CamperBehaviour.Agent.Mosquito.VictoryRewards.mean": {
            "value": 73.0,
            "min": 3.0,
            "max": 83.0,
            "count": 308
        },
        "CamperBehaviour.Agent.Mosquito.VictoryRewards.sum": {
            "value": 73.0,
            "min": 3.0,
            "max": 83.0,
            "count": 308
        },
        "CamperBehaviour.Agent.Mosquito.DefeatRewards.mean": {
            "value": 217.0,
            "min": 0.0,
            "max": 234.0,
            "count": 308
        },
        "CamperBehaviour.Agent.Mosquito.DefeatRewards.sum": {
            "value": 217.0,
            "min": 0.0,
            "max": 234.0,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.ObjectivePickUpRewards.mean": {
            "value": 481.19976806640625,
            "min": 4.0,
            "max": 484.7997741699219,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.ObjectivePickUpRewards.sum": {
            "value": 481.19976806640625,
            "min": 4.0,
            "max": 484.7997741699219,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.ObjectiveDropOffRewards.mean": {
            "value": 417.9998474121094,
            "min": 1.2000000476837158,
            "max": 429.9998779296875,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.ObjectiveDropOffRewards.sum": {
            "value": 417.9998474121094,
            "min": 1.2000000476837158,
            "max": 429.9998779296875,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.DeathRewards.mean": {
            "value": -171.5,
            "min": -183.5,
            "max": -4.5,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.DeathRewards.sum": {
            "value": -171.5,
            "min": -183.5,
            "max": -4.5,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.VictoryRewards.mean": {
            "value": 217.0,
            "min": 0.0,
            "max": 234.0,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.VictoryRewards.sum": {
            "value": 217.0,
            "min": 0.0,
            "max": 234.0,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.DefeatRewards.mean": {
            "value": 73.0,
            "min": 3.0,
            "max": 83.0,
            "count": 308
        },
        "CamperBehaviour.Agent.Camper.DefeatRewards.sum": {
            "value": 73.0,
            "min": 3.0,
            "max": 83.0,
            "count": 308
        },
        "CamperBehaviour.Environment.LessonNumber.training_stage_index.mean": {
            "value": 4.0,
            "min": 3.0,
            "max": 4.0,
            "count": 308
        },
        "CamperBehaviour.Environment.LessonNumber.training_stage_index.sum": {
            "value": 4.0,
            "min": 3.0,
            "max": 4.0,
            "count": 308
        },
        "CamperBehaviour.Environment.EpisodeLength.mean": {
            "value": 1610.5862068965516,
            "min": 309.46283448959366,
            "max": 3270.8888888888887,
            "count": 308
        },
        "CamperBehaviour.Environment.EpisodeLength.sum": {
            "value": 46707.0,
            "min": 6312.0,
            "max": 476107.0,
            "count": 308
        },
        "CamperBehaviour.Step.mean": {
            "value": 22679120.0,
            "min": 4259572.0,
            "max": 22679120.0,
            "count": 308
        },
        "CamperBehaviour.Step.sum": {
            "value": 22679120.0,
            "min": 4259572.0,
            "max": 22679120.0,
            "count": 308
        },
        "CamperBehaviour.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": -0.0070676966570317745,
            "min": -14.047111511230469,
            "max": 0.09796860814094543,
            "count": 308
        },
        "CamperBehaviour.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": -0.5230095386505127,
            "min": -1151.8631591796875,
            "max": 16.108827590942383,
            "count": 308
        },
        "CamperBehaviour.Policy.ExtrinsicValueEstimate.mean": {
            "value": -0.0015980154275894165,
            "min": -14.155769348144531,
            "max": 0.0931100994348526,
            "count": 308
        },
        "CamperBehaviour.Policy.ExtrinsicValueEstimate.sum": {
            "value": -0.11825314164161682,
            "min": -1134.1357421875,
            "max": 15.888696670532227,
            "count": 308
        },
        "CamperBehaviour.Environment.CumulativeReward.mean": {
            "value": -0.24862068897948184,
            "min": -115.50725048184395,
            "max": 0.2882352869081147,
            "count": 308
        },
        "CamperBehaviour.Environment.CumulativeReward.sum": {
            "value": -7.209999980404973,
            "min": -4620.290019273758,
            "max": 4.899999877437949,
            "count": 308
        },
        "CamperBehaviour.Policy.ExtrinsicReward.mean": {
            "value": -0.9735378263325527,
            "min": -271.9561385422945,
            "max": 1.5944453369487415,
            "count": 308
        },
        "CamperBehaviour.Policy.ExtrinsicReward.sum": {
            "value": -28.232596963644028,
            "min": -10878.24554169178,
            "max": 113.28003294765949,
            "count": 308
        },
        "CamperBehaviour.Self-play.ELO.mean": {
            "value": 2171.4181857315725,
            "min": 1621.4946092523069,
            "max": 2183.1028521023845,
            "count": 308
        },
        "CamperBehaviour.Self-play.ELO.sum": {
            "value": 34742.69097170516,
            "min": 10909.446646001905,
            "max": 335360.4054432141,
            "count": 308
        },
        "CamperBehaviour.Environment.GroupCumulativeReward.mean": {
            "value": 0.6438248911872506,
            "min": -2.0881290520940508,
            "max": 1.712145889408133,
            "count": 308
        },
        "CamperBehaviour.Environment.GroupCumulativeReward.sum": {
            "value": 10.30119825899601,
            "min": -84.58071771264076,
            "max": 253.0080909729004,
            "count": 308
        },
        "CamperBehaviour.Losses.PolicyLoss.mean": {
            "value": 0.022740264904798205,
            "min": 0.014720217136976618,
            "max": 0.023965010484680534,
            "count": 263
        },
        "CamperBehaviour.Losses.PolicyLoss.sum": {
            "value": 0.06822079471439461,
            "min": 0.01774749126518145,
            "max": 0.06822079471439461,
            "count": 263
        },
        "CamperBehaviour.Losses.ValueLoss.mean": {
            "value": 0.006028130539184944,
            "min": 0.0022757569772445343,
            "max": 4.3970042014122015,
            "count": 263
        },
        "CamperBehaviour.Losses.ValueLoss.sum": {
            "value": 0.018084391617554832,
            "min": 0.003494645468890667,
            "max": 13.04355317786336,
            "count": 263
        },
        "CamperBehaviour.Losses.BaselineLoss.mean": {
            "value": 0.007004720050785126,
            "min": 0.003020547801685153,
            "max": 15.854364538192748,
            "count": 263
        },
        "CamperBehaviour.Losses.BaselineLoss.sum": {
            "value": 0.02101416015235538,
            "min": 0.004123442245059107,
            "max": 31.708729076385495,
            "count": 263
        },
        "CamperBehaviour.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0002999999999999999,
            "max": 0.0003000000000000001,
            "count": 263
        },
        "CamperBehaviour.Policy.LearningRate.sum": {
            "value": 0.0009,
            "min": 0.0003,
            "max": 0.0009000000000000002,
            "count": 263
        },
        "CamperBehaviour.Policy.Epsilon.mean": {
            "value": 0.19999999999999996,
            "min": 0.19999999999999987,
            "max": 0.19999999999999998,
            "count": 263
        },
        "CamperBehaviour.Policy.Epsilon.sum": {
            "value": 0.5999999999999999,
            "min": 0.1999999999999999,
            "max": 0.6,
            "count": 263
        },
        "CamperBehaviour.Policy.Beta.mean": {
            "value": 0.005,
            "min": 0.004999999999999999,
            "max": 0.005000000000000001,
            "count": 263
        },
        "CamperBehaviour.Policy.Beta.sum": {
            "value": 0.015,
            "min": 0.004999999999999999,
            "max": 0.015000000000000003,
            "count": 263
        },
        "CamperBehaviour.IsTraining.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 1.0,
            "count": 308
        },
        "CamperBehaviour.IsTraining.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 1.0,
            "count": 308
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1630198228",
        "python_version": "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]",
        "command_line_arguments": "D:\\Miniconda\\Scripts\\mlagents-learn config/TrainingConfigFinal.yaml --run-id=CurriculumTest2.0 --resume",
        "mlagents_version": "0.27.0",
        "mlagents_envs_version": "0.27.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.7.1+cu110",
        "numpy_version": "1.19.5",
        "end_time_seconds": "1630266986"
    },
    "total": 68759.24675080001,
    "count": 1,
    "self": 0.013028600020334125,
    "children": {
        "run_training.setup": {
            "total": 0.22993679999999994,
            "count": 1,
            "self": 0.22993679999999994
        },
        "TrainerController.start_learning": {
            "total": 68759.0037854,
            "count": 1,
            "self": 24.213730099843815,
            "children": {
                "TrainerController._reset_env": {
                    "total": 47.595509599982094,
                    "count": 84,
                    "self": 47.595509599982094
                },
                "TrainerController.advance": {
                    "total": 68686.67187880017,
                    "count": 496863,
                    "self": 26.32708670386637,
                    "children": {
                        "env_step": {
                            "total": 51230.53362109981,
                            "count": 496863,
                            "self": 46696.31031319905,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 4526.929746999008,
                                    "count": 496863,
                                    "self": 91.42156529592285,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 4435.508181703085,
                                            "count": 969648,
                                            "self": 973.6527769971226,
                                            "children": {
                                                "TorchPolicy.sample_actions": {
                                                    "total": 3461.8554047059624,
                                                    "count": 969648,
                                                    "self": 3461.8554047059624
                                                }
                                            }
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 7.293560901747027,
                                    "count": 496862,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 68728.31480430218,
                                            "count": 496862,
                                            "is_parallel": true,
                                            "self": 25725.035553804177,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 1.4881068000102875,
                                                    "count": 168,
                                                    "is_parallel": true,
                                                    "self": 0.1033248999767915,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 1.384781900033496,
                                                            "count": 1176,
                                                            "is_parallel": true,
                                                            "self": 1.384781900033496
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 43001.79114369799,
                                                    "count": 496862,
                                                    "is_parallel": true,
                                                    "self": 1435.597604499475,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 707.2221471998014,
                                                            "count": 496862,
                                                            "is_parallel": true,
                                                            "self": 707.2221471998014
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 32028.678161300777,
                                                            "count": 496862,
                                                            "is_parallel": true,
                                                            "self": 32028.678161300777
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 8830.293230697942,
                                                            "count": 993724,
                                                            "is_parallel": true,
                                                            "self": 632.0286099970181,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 8198.264620700924,
                                                                    "count": 6956068,
                                                                    "is_parallel": true,
                                                                    "self": 8198.264620700924
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 17429.811170996498,
                            "count": 993724,
                            "self": 241.0700059961564,
                            "children": {
                                "process_trajectory": {
                                    "total": 2992.775902000403,
                                    "count": 993724,
                                    "self": 2978.3112970003963,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 14.46460500000694,
                                            "count": 49,
                                            "self": 14.46460500000694
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 14195.96526299994,
                                    "count": 1202,
                                    "self": 9166.310535099117,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 604.9989760003707,
                                            "count": 29285,
                                            "self": 604.9989760003707
                                        },
                                        "TorchPOCAOptimizer.update": {
                                            "total": 4424.655751900453,
                                            "count": 37235,
                                            "self": 4424.655751900453
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 1.0999938240274787e-06,
                    "count": 1,
                    "self": 1.0999938240274787e-06
                },
                "TrainerController._save_models": {
                    "total": 0.5226658000028692,
                    "count": 1,
                    "self": 0.027864000003319234,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.49480179999955,
                            "count": 2,
                            "self": 0.49480179999955
                        }
                    }
                }
            }
        }
    }
}