{
    "name": "root",
    "gauges": {
        "CamperBehaviour.Policy.Entropy.mean": {
            "value": 1.024209976196289,
            "min": 0.9724951386451721,
            "max": 3.988877296447754,
            "count": 134
        },
        "CamperBehaviour.Policy.Entropy.sum": {
            "value": 59762.65234375,
            "min": 58291.359375,
            "max": 314638.65625,
            "count": 134
        },
        "CamperBehaviour.TimeOutRewards.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 134
        },
        "CamperBehaviour.TimeOutRewards.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 134
        },
        "CamperBehaviour.CollisionRewards.mean": {
            "value": -515.9902954101562,
            "min": -669.7718505859375,
            "max": -1.8899985551834106,
            "count": 134
        },
        "CamperBehaviour.CollisionRewards.sum": {
            "value": -515.9902954101562,
            "min": -669.7718505859375,
            "max": -1.8899985551834106,
            "count": 134
        },
        "CamperBehaviour.Agent.Mosquito.KillRewards.mean": {
            "value": 911.0,
            "min": 0.0,
            "max": 917.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Mosquito.KillRewards.sum": {
            "value": 911.0,
            "min": 0.0,
            "max": 917.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Mosquito.VictoryRewards.mean": {
            "value": 156.0,
            "min": 0.0,
            "max": 181.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Mosquito.VictoryRewards.sum": {
            "value": 156.0,
            "min": 0.0,
            "max": 181.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Mosquito.DefeatRewards.mean": {
            "value": 656.0,
            "min": 0.0,
            "max": 656.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Mosquito.DefeatRewards.sum": {
            "value": 656.0,
            "min": 0.0,
            "max": 656.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.ObjectivePickUpRewards.mean": {
            "value": 1271.0,
            "min": 1.0,
            "max": 1271.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.ObjectivePickUpRewards.sum": {
            "value": 1271.0,
            "min": 1.0,
            "max": 1271.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.ObjectiveDropOffRewards.mean": {
            "value": 2197.0,
            "min": 0.0,
            "max": 2197.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.ObjectiveDropOffRewards.sum": {
            "value": 2197.0,
            "min": 0.0,
            "max": 2197.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.DeathRewards.mean": {
            "value": -455.5,
            "min": -458.5,
            "max": 0.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.DeathRewards.sum": {
            "value": -455.5,
            "min": -458.5,
            "max": 0.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.VictoryRewards.mean": {
            "value": 656.0,
            "min": 0.0,
            "max": 656.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.VictoryRewards.sum": {
            "value": 656.0,
            "min": 0.0,
            "max": 656.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.DefeatRewards.mean": {
            "value": 156.0,
            "min": 0.0,
            "max": 181.0,
            "count": 134
        },
        "CamperBehaviour.Agent.Camper.DefeatRewards.sum": {
            "value": 156.0,
            "min": 0.0,
            "max": 181.0,
            "count": 134
        },
        "CamperBehaviour.Environment.EpisodeLength.mean": {
            "value": 68.57625272331154,
            "min": 58.47175421209118,
            "max": 2142.4,
            "count": 134
        },
        "CamperBehaviour.Environment.EpisodeLength.sum": {
            "value": 62953.0,
            "min": 16568.0,
            "max": 87404.0,
            "count": 134
        },
        "CamperBehaviour.Step.mean": {
            "value": 8039981.0,
            "min": 59605.0,
            "max": 8039981.0,
            "count": 134
        },
        "CamperBehaviour.Step.sum": {
            "value": 8039981.0,
            "min": 59605.0,
            "max": 8039981.0,
            "count": 134
        },
        "CamperBehaviour.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": 4.824212551116943,
            "min": -3.4116287231445312,
            "max": 4.9916510581970215,
            "count": 134
        },
        "CamperBehaviour.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": 4443.099609375,
            "min": -272.2292175292969,
            "max": 4791.98486328125,
            "count": 134
        },
        "CamperBehaviour.Policy.ExtrinsicValueEstimate.mean": {
            "value": 4.855503559112549,
            "min": -3.7901418209075928,
            "max": 5.00941276550293,
            "count": 134
        },
        "CamperBehaviour.Policy.ExtrinsicValueEstimate.sum": {
            "value": 4471.9189453125,
            "min": -288.05078125,
            "max": 4752.2294921875,
            "count": 134
        },
        "CamperBehaviour.Environment.CumulativeReward.mean": {
            "value": -1.9526030222981157,
            "min": -491.66691944473666,
            "max": -0.6505606273428174,
            "count": 134
        },
        "CamperBehaviour.Environment.CumulativeReward.sum": {
            "value": -1794.4421774919683,
            "min": -12798.754341125488,
            "max": -84.57288155456627,
            "count": 134
        },
        "CamperBehaviour.Policy.ExtrinsicReward.mean": {
            "value": 20.58044969631,
            "min": -1135.44546934178,
            "max": 28.698685216789062,
            "count": 134
        },
        "CamperBehaviour.Policy.ExtrinsicReward.sum": {
            "value": 18913.433270908892,
            "min": -26558.051864624023,
            "max": 22092.154204837978,
            "count": 134
        },
        "CamperBehaviour.Environment.GroupCumulativeReward.mean": {
            "value": 29.557692307692307,
            "min": 14.714285714285714,
            "max": 32.10526315789474,
            "count": 134
        },
        "CamperBehaviour.Environment.GroupCumulativeReward.sum": {
            "value": 16907.0,
            "min": 88.0,
            "max": 20101.0,
            "count": 134
        },
        "CamperBehaviour.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 134
        },
        "CamperBehaviour.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 134
        },
        "CamperBehaviour.Losses.PolicyLoss.mean": {
            "value": 0.01683974094533672,
            "min": 0.012245487681745241,
            "max": 0.020435638322184483,
            "count": 133
        },
        "CamperBehaviour.Losses.PolicyLoss.sum": {
            "value": 0.050519222836010166,
            "min": 0.017951683409780532,
            "max": 0.06130691496655345,
            "count": 133
        },
        "CamperBehaviour.Losses.ValueLoss.mean": {
            "value": 8.455844587749906,
            "min": 0.24109319200118384,
            "max": 8.682333366076152,
            "count": 133
        },
        "CamperBehaviour.Losses.ValueLoss.sum": {
            "value": 25.367533763249718,
            "min": 0.4967692824701468,
            "max": 25.805312840143838,
            "count": 133
        },
        "CamperBehaviour.Losses.BaselineLoss.mean": {
            "value": 9.940204721026952,
            "min": 0.24874253285427889,
            "max": 11.62493759017807,
            "count": 133
        },
        "CamperBehaviour.Losses.BaselineLoss.sum": {
            "value": 29.820614163080855,
            "min": 0.49748506570855777,
            "max": 31.352464087804158,
            "count": 133
        },
        "CamperBehaviour.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.00030000000000000003,
            "count": 133
        },
        "CamperBehaviour.Policy.LearningRate.sum": {
            "value": 0.0009,
            "min": 0.00030000000000000003,
            "max": 0.0009,
            "count": 133
        },
        "CamperBehaviour.Policy.Epsilon.mean": {
            "value": 0.10000000000000003,
            "min": 0.09999999999999998,
            "max": 0.10000000000000003,
            "count": 133
        },
        "CamperBehaviour.Policy.Epsilon.sum": {
            "value": 0.3000000000000001,
            "min": 0.09999999999999998,
            "max": 0.3000000000000001,
            "count": 133
        },
        "CamperBehaviour.Policy.Beta.mean": {
            "value": 0.0010000000000000002,
            "min": 0.001,
            "max": 0.0010000000000000005,
            "count": 133
        },
        "CamperBehaviour.Policy.Beta.sum": {
            "value": 0.003000000000000001,
            "min": 0.0010000000000000005,
            "max": 0.003000000000000001,
            "count": 133
        },
        "MosquitoBehaviour.Policy.Entropy.mean": {
            "value": 2.1307435035705566,
            "min": 2.129922866821289,
            "max": 3.9884276390075684,
            "count": 60
        },
        "MosquitoBehaviour.Policy.Entropy.sum": {
            "value": 128390.078125,
            "min": 128390.078125,
            "max": 255259.375,
            "count": 60
        },
        "MosquitoBehaviour.TimeOutRewards.mean": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 60
        },
        "MosquitoBehaviour.TimeOutRewards.sum": {
            "value": 0.0,
            "min": 0.0,
            "max": 0.0,
            "count": 60
        },
        "MosquitoBehaviour.CollisionRewards.mean": {
            "value": -517.3809204101562,
            "min": -536.5714111328125,
            "max": -3.299997329711914,
            "count": 60
        },
        "MosquitoBehaviour.CollisionRewards.sum": {
            "value": -517.3809204101562,
            "min": -536.5714111328125,
            "max": -3.299997329711914,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Mosquito.KillRewards.mean": {
            "value": 866.0,
            "min": 2.0,
            "max": 950.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Mosquito.KillRewards.sum": {
            "value": 866.0,
            "min": 2.0,
            "max": 950.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Mosquito.VictoryRewards.mean": {
            "value": 157.0,
            "min": 0.0,
            "max": 182.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Mosquito.VictoryRewards.sum": {
            "value": 157.0,
            "min": 0.0,
            "max": 182.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Mosquito.DefeatRewards.mean": {
            "value": 626.0,
            "min": 0.0,
            "max": 639.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Mosquito.DefeatRewards.sum": {
            "value": 626.0,
            "min": 0.0,
            "max": 639.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.ObjectivePickUpRewards.mean": {
            "value": 1202.0,
            "min": 1.0,
            "max": 1237.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.ObjectivePickUpRewards.sum": {
            "value": 1202.0,
            "min": 1.0,
            "max": 1237.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.ObjectiveDropOffRewards.mean": {
            "value": 2125.0,
            "min": 0.0,
            "max": 2144.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.ObjectiveDropOffRewards.sum": {
            "value": 2125.0,
            "min": 0.0,
            "max": 2144.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.DeathRewards.mean": {
            "value": -433.0,
            "min": -475.0,
            "max": -1.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.DeathRewards.sum": {
            "value": -433.0,
            "min": -475.0,
            "max": -1.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.VictoryRewards.mean": {
            "value": 626.0,
            "min": 0.0,
            "max": 639.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.VictoryRewards.sum": {
            "value": 626.0,
            "min": 0.0,
            "max": 639.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.DefeatRewards.mean": {
            "value": 157.0,
            "min": 0.0,
            "max": 182.0,
            "count": 60
        },
        "MosquitoBehaviour.Agent.Camper.DefeatRewards.sum": {
            "value": 157.0,
            "min": 0.0,
            "max": 182.0,
            "count": 60
        },
        "MosquitoBehaviour.Environment.EpisodeLength.mean": {
            "value": 79.7287899860918,
            "min": 78.02635046113306,
            "max": 3363.25,
            "count": 60
        },
        "MosquitoBehaviour.Environment.EpisodeLength.sum": {
            "value": 57325.0,
            "min": 7082.0,
            "max": 80931.0,
            "count": 60
        },
        "MosquitoBehaviour.Step.mean": {
            "value": 3599950.0,
            "min": 59088.0,
            "max": 3599950.0,
            "count": 60
        },
        "MosquitoBehaviour.Step.sum": {
            "value": 3599950.0,
            "min": 59088.0,
            "max": 3599950.0,
            "count": 60
        },
        "MosquitoBehaviour.Policy.ExtrinsicValueEstimate.mean": {
            "value": -0.12628173828125,
            "min": -0.2976091504096985,
            "max": -0.023818083107471466,
            "count": 60
        },
        "MosquitoBehaviour.Policy.ExtrinsicValueEstimate.sum": {
            "value": -91.04913330078125,
            "min": -180.6260986328125,
            "max": -1.476721167564392,
            "count": 60
        },
        "MosquitoBehaviour.Environment.CumulativeReward.mean": {
            "value": -0.7104231429199516,
            "min": -16.142687380313873,
            "max": -0.7104231429199516,
            "count": 60
        },
        "MosquitoBehaviour.Environment.CumulativeReward.sum": {
            "value": -510.7942397594452,
            "min": -1673.4284423589706,
            "max": -11.044295310974121,
            "count": 60
        },
        "MosquitoBehaviour.Policy.ExtrinsicReward.mean": {
            "value": -0.7104231429199516,
            "min": -16.142687380313873,
            "max": -0.7104231429199516,
            "count": 60
        },
        "MosquitoBehaviour.Policy.ExtrinsicReward.sum": {
            "value": -510.7942397594452,
            "min": -1673.4284423589706,
            "max": -11.044295310974121,
            "count": 60
        },
        "MosquitoBehaviour.Losses.PolicyLoss.mean": {
            "value": 0.01563531485800114,
            "min": 0.013437348262717325,
            "max": 0.019640890542521244,
            "count": 60
        },
        "MosquitoBehaviour.Losses.PolicyLoss.sum": {
            "value": 0.04690594457400342,
            "min": 0.017777261779540116,
            "max": 0.05892267162756373,
            "count": 60
        },
        "MosquitoBehaviour.Losses.ValueLoss.mean": {
            "value": 1.2512311842706467,
            "min": 0.018766623611251514,
            "max": 1.2954272468884787,
            "count": 60
        },
        "MosquitoBehaviour.Losses.ValueLoss.sum": {
            "value": 3.7536935528119404,
            "min": 0.018766623611251514,
            "max": 3.886281740665436,
            "count": 60
        },
        "MosquitoBehaviour.Policy.LearningRate.mean": {
            "value": 9.999999999999998e-05,
            "min": 9.999999999999998e-05,
            "max": 0.00010000000000000003,
            "count": 60
        },
        "MosquitoBehaviour.Policy.LearningRate.sum": {
            "value": 0.0002999999999999999,
            "min": 0.00010000000000000003,
            "max": 0.00030000000000000003,
            "count": 60
        },
        "MosquitoBehaviour.Policy.Epsilon.mean": {
            "value": 0.10000000000000003,
            "min": 0.09999999999999996,
            "max": 0.10000000000000003,
            "count": 60
        },
        "MosquitoBehaviour.Policy.Epsilon.sum": {
            "value": 0.3000000000000001,
            "min": 0.09999999999999996,
            "max": 0.3000000000000001,
            "count": 60
        },
        "MosquitoBehaviour.Policy.Beta.mean": {
            "value": 0.0010000000000000002,
            "min": 0.001,
            "max": 0.0010000000000000002,
            "count": 60
        },
        "MosquitoBehaviour.Policy.Beta.sum": {
            "value": 0.003000000000000001,
            "min": 0.001,
            "max": 0.003000000000000001,
            "count": 60
        },
        "MosquitoBehaviour.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 60
        },
        "MosquitoBehaviour.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 60
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1629627081",
        "python_version": "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]",
        "command_line_arguments": "D:\\Miniconda\\Scripts\\mlagents-learn config/TrainingConfig.yaml --run-id=TrainingResults",
        "mlagents_version": "0.27.0",
        "mlagents_envs_version": "0.27.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "1.7.1+cu110",
        "numpy_version": "1.19.5",
        "end_time_seconds": "1629643588"
    },
    "total": 16507.0414091,
    "count": 1,
    "self": 0.007777300001180265,
    "children": {
        "run_training.setup": {
            "total": 0.21379729999999997,
            "count": 1,
            "self": 0.21379729999999997
        },
        "TrainerController.start_learning": {
            "total": 16506.8198345,
            "count": 1,
            "self": 3.3715662997237814,
            "children": {
                "TrainerController._reset_env": {
                    "total": 10.2657311,
                    "count": 1,
                    "self": 10.2657311
                },
                "TrainerController.advance": {
                    "total": 16492.819503800274,
                    "count": 150960,
                    "self": 4.778021599729982,
                    "children": {
                        "env_step": {
                            "total": 10682.991467500175,
                            "count": 150960,
                            "self": 9626.982193300368,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 1053.9943596997973,
                                    "count": 150960,
                                    "self": 19.052234800076803,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 1034.9421248997205,
                                            "count": 239052,
                                            "self": 227.4620015993553,
                                            "children": {
                                                "TorchPolicy.sample_actions": {
                                                    "total": 807.4801233003652,
                                                    "count": 239052,
                                                    "self": 807.4801233003652
                                                }
                                            }
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 2.0149145000099846,
                                    "count": 150959,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 16494.166840500002,
                                            "count": 150959,
                                            "is_parallel": true,
                                            "self": 7660.907029299824,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.017311199999999083,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.0006209999999970961,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.016690200000001987,
                                                            "count": 12,
                                                            "is_parallel": true,
                                                            "self": 0.016690200000001987
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 8833.242500000179,
                                                    "count": 150959,
                                                    "is_parallel": true,
                                                    "self": 261.7658442006341,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 145.17504590005177,
                                                            "count": 150959,
                                                            "is_parallel": true,
                                                            "self": 145.17504590005177
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 6897.799082799459,
                                                            "count": 150959,
                                                            "is_parallel": true,
                                                            "self": 6897.799082799459
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 1528.5025271000345,
                                                            "count": 301918,
                                                            "is_parallel": true,
                                                            "self": 80.29824899977439,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 1448.2042781002601,
                                                                    "count": 1811508,
                                                                    "is_parallel": true,
                                                                    "self": 1448.2042781002601
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 5805.0500147003695,
                            "count": 301918,
                            "self": 29.401935399981085,
                            "children": {
                                "process_trajectory": {
                                    "total": 2181.5900055003576,
                                    "count": 301918,
                                    "self": 2177.9930087003554,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 3.5969968000022163,
                                            "count": 23,
                                            "self": 3.5969968000022163
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 3594.058073800031,
                                    "count": 557,
                                    "self": 2420.197175800063,
                                    "children": {
                                        "TorchPOCAOptimizer.update": {
                                            "total": 1036.0791999999803,
                                            "count": 11676,
                                            "self": 1036.0791999999803
                                        },
                                        "TorchPPOOptimizer.update": {
                                            "total": 137.78169799998727,
                                            "count": 5253,
                                            "self": 137.78169799998727
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 1.7999991541728377e-06,
                    "count": 1,
                    "self": 1.7999991541728377e-06
                },
                "TrainerController._save_models": {
                    "total": 0.3630315000009432,
                    "count": 1,
                    "self": 0.019345000004250323,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.3436864999966929,
                            "count": 2,
                            "self": 0.3436864999966929
                        }
                    }
                }
            }
        }
    }
}